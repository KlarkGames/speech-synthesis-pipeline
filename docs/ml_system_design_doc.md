# ML System Design Document

## Предисловие

Данный проект ведется очень сумбурно. За ~полгода работы с заказчиком я не видел ни одного ТЗ, конкретного требования. Отдельно эта проблема касается нефункциональных требований. Соответственно, некоторые вопросы, по типу скорости модели или критериев оценки качества синтеза, будут опущены, по причине отсутствия ответа со стороны заказчика.

## Содержание

- [ML System Design Document](#ml-system-design-document)
  - [Предисловие](#предисловие)
  - [Содержание](#содержание)
  - [1. Цели и предпосылки](#1-цели-и-предпосылки)
    - [1.1. Бизнес-цели](#11-бизнес-цели)
  - [2. Требования к решению](#2-требования-к-решению)
    - [2.1. Функциональные требования](#21-функциональные-требования)
    - [2.2. Нефункциональные требования](#22-нефункциональные-требования)
    - [2.3. Ограничение ответственности команды разработки](#23-ограничение-ответственности-команды-разработки)
    - [2.4. Блоксхема решения](#24-блоксхема-решения)


## 1. Цели и предпосылки

### 1.1. Бизнес-цели

**Бизнес-цель** - продемонстрировать современность имеющихся в компании технологий и подходов перед заказчиком. Актуализировать технологии. 

Разрабатываемая командой часть проекта является **Emotional Speech Synthesis (Дальше ESS)** модель для озвучки сгенерированных LLM диалогов. Озвучка ожидается "на лету", т.е. нет возможности озвучить диалоги заранее.

У разрабатывающей команды есть наработки в области ESS, данный проект позволит актуализировать имеющиеся подходы и технологии. 

Успешным результатом **с точки зрения заказчика** является разворачиваемая на инфраструктуре заказчика и силами его разработчиков ESS модель. Качество модели должно субъективно удовлетворять представителей заказчика.

Успешным результатом **c точки зрения разрабатывающей команды** является отданный заказчику checkpoint модели и GitLab репозиторий с инструкциями по запуску. Дополнительно упомянутые выше артефакты должны удовлетворять всем оговоренным функциональным и нефункциональным требованиям.

## 2. Требования к решению

### 2.1. Функциональные требования

1. Пользователь должен при имеющихся тексте и идентификаторах голоса и эмоции получить озвучку текста с соответствующими голосом и эмоцией.
2. Пользователь может вместо идентификатора голоса и эмоции дать на вход аудиофайл, содержащий пример требуемого голоса и эмоции. 
3. Команда разработки со стороны заказчика должна иметь возможность запустить Docker контейнер с моделью и чекпоинтом и повторить действия из пункта 1.

### 2.2. Нефункциональные требования

1. На инфраструктуре команды разработчиков разрабатываемый контейнер с моделью должен держать 100 RPS.
2. Текст должен быть представлен без дополнительных тегов (По типу Markdown, HTML), на английском языке в нижнем и верхнем регистре с числами и символами препинания входящими в список **!?.,"'()-`+**
3. Идентификаторы голоса и эмоции ожидаются в виде текстовой строки в нижнем регистре с символами английского алфавита и числами. При отсутствии требуемой эмоции или голоса - контейнер должен вернуть ошибку с перечнем имеющихся эмоций и голосов. 
4. Пример аудио с голосом и эмоцией ожидается с частотой дискретизации 44.1 kHZ с 1 каналом (mono) и Encoding'ом 16-bit Signed Integer PCM.
5. Если входные данные не соответствуют пунктам 2-4, несоответствия должны обрабатываться в контейнере модели. 
6. Возвращаемое аудио так же должно соответствовать пункту 4.
7. Качество синтезируемого аудио должно удовлетворять сторону заказчика. 
8. Синтезируемое аудио должно быть без заметных артефактов, по типу скрежета, писка, робовойса, белого шума.

### 2.3. Ограничение ответственности команды разработки

Команда разработки предоставляет Docker контейнер с моделью и чекпоинты для запуска. Команда разработки вносит изменения в контейнер, если они не удовлетворяют требованиям. Однако команда разработки не отвечает за интеграцию этого контейнера в конечный продукт заказчика. 

Команда разработки может настоять на проведение Mean Opinion Score теста (MOS) для объективной оценки качества синтеза голоса в случаях разногласий в субъективной оценки результатов. 

Команда разработки не предоставляет заказчику информацию о дополнительных данных, используемых для обучения, и о методах/пайплайнах обработки и обучения.

### 2.4. Блоксхема решения

![alt text](<images/Model Workflow.drawio.png>)

## 3. Данные

### 3.1. Требования к данным

Данные должны удовлетворять следующему:
- Данные представлены парой аудиофайл - текстовый файл. В аудиофайле содержится голос, произносящий текст, хранящийся в текстовом файле. 
- Качество голоса на аудио должно соответствовать тому, которое ожидается в качестве синтезируемого от модели.
- Датасет должен иметь однозначную индикацию, к какому спикеру пренадлежит голос на аудиофайлах. 
- Аудиофайлы должны иметь частоту дискретизации не менее 16kHZ
- Допускается использование разной канальности и методов кодирования. Рекомендуется использовать аудио с 1 каналом (mono) и Encoding'ом 16-bit Signed Integer PCM.
- Текст должен быть представлен без дополнительных тегов (По типу Markdown, HTML), на английском языке в нижнем и верхнем регистре с числами и символами препинания входящими в список **.,'`**. Другие символы будут удалены или заменены на их аналоги (Например ! -> .)
- Знаки препинания должны быть раставлены в местах, где присутствует характерная пауза. Точка - длинная пауза (>0.3с), запятая - которкая (>0.15с).

### 3.2. Стандартизованный формат хранения данных

Все данные должны быть приведены к одному формату, представленному ниже:

```
all_datasets/                # Главная папка с несколькими мультиспикерными датасетами
│
├── metadata.csv             # Главная метадата, содержащая пути ко ВСЕМ wav файлам в текущем датасете
│
├── EmoV_DB/                 # Папка мультиспикерного датасета EmoV_DB
|   ├── metadata.csv         # Метадата содержащая пути ко всем файлам внутри датасета EmoV_DB
│   ├── speaker_1/           # Папка спикера
│   │   └── wavs/            # Папка с аудиофайлами или их подкатегориями
│   │       ├── file1.wav
│   │       └── ...
│   ├── speaker_2/
│   |   └── wavs/
│   |       ├── subfolder/   # Подпапка с аудиофайлами, например, по книгам (опционально - если есть такое разделение)
│   |       |   ├── file3.wav
│   |       |   └── ...
|   |       ├── file11.wav
|   |       └── ...
|   ├── ...
|   └── wavs                 # Папка с аудиофайлами без спикеров
|       ├── no_speaker1.wav
|       └── ...
|
└── MLS/                     # Папка мультиспикерного датасета MLS
    ├── metadata.csv
    ├── speaker_1/
    │   └── wavs/
    │       ├── file1.wav
    |       └── ...
    ├── speaker_2/
    |   └── wavs/
    |       ├── file2.wav
    |       └── ...
    └── wavs                   # Папка с аудиофайлами без спикеров
        ├── no_speaker1.wav
        └── ...
```

Файл metadata.csv должен соответствовать следующей структуре (all_datasets/EmoV_DB/metadata.csv):

| path_to_wav                        | speaker_id | text                   |
| ---------------------------------- | ---------- | ---------------------- |
| speaker_1/wavs/file1.wav           | 0          | Hello world            |
| speaker_2/wavs/subfolder/file3.wav | 1          | It's me, Mario         |
| speaker_2/wavs/file11.wav          | 1          | Mario time!            |
| wavs/no_speaker1.wav               | -1         | Keep you waiting, huh? |

**Для аудио с отсутствующей информацией о спикере проставляется ID -1.**

*Speaker_id* и *path_to_wav* являются локальными в пределах датасета. Пример для сравнения (all_datasets/metadata.csv)

| path_to_wav                                | speaker_id | text                   |
| ------------------------------------------ | ---------- | ---------------------- |
| EmoV_DB/speaker_1/wavs/file1.wav           | 0          | Hello world            |
| EmoV_DB/speaker_2/wavs/subfolder/file3.wav | 1          | It's me, Mario         |
| EmoV_DB/speaker_2/wavs/file11.wav          | 1          | Mario time!            |
| EmoV_DB/wavs/no_speaker1.wav               | -1         | Keep you waiting, huh? |
| MLS/speaker_1/wavs/file1.wav               | 2          | Nanomachines, son!     |
| MLS/speaker_2/wavs/file2.wav               | 3          | Leeroooooy Jenkins!    |
| MLS/wavs/no_speaker1.wav                   | -1         | Very dramatic, Uther.  |

Так же стоит учитывать следующие замечания:
- Аудио файлы **должны быть** в формате .wav.
- Аудио файлы **должны быть** в локальной wavs директории.
- Подразумевается что **нет несоответствий** между информацией в metadata.csv и соответствующей структуре директорий.
- Аудиофайлы **должны быть** одного sample rate
- Аудиофайлы **представлены с 1 каналом (mono).**
- .csv файлы **должны быть** сохранены с параметрами **sep='|', index=False**
- Колонки для .csv файлов **должны быть** в порядке *path_to_wav | text | speaker_id* для поддержания обратной совместимости

### 3.4. Источники данных

*Большая часть данных скрыта из-за NDA*

#### Открытые данные

- Multilingual LibriSpeech - Большой датасет с записями аудиокниг с соответствующим текстом. Размечен по спикерам, имеет 44 тысяч часов записи, равномерно распределенной между мужскими и женскими голосами.
- EmoV_DB - Небольшой датасет с записями актеров, которые зачитывают текст в заданной эмоции.  

Подробнее о датасетах и их форматах можно посмотреть в [datasets.md](datasets.md)

### 3.5. Обработка данных

Данные обрабатываются в следующем порядке:
- Все датасеты приводятся к [стандартизированному формату](#32-стандартизованный-формат-хранения-данных).
- Датасеты, для которых качество аудио субъективно неудовлетворительное, обрабатываются с помощью Resemble-Enhancer'а. Данный инструмент используется для восстановление качества голоса с старых записей.
- Датасеты проходят фильтрацию с помощью Automatic Speech Recognition модели. Если WER и CER между распознанными и размеченными текстами превышают заданные пороги - данные семплы удаляются из датасета. (Удаляются не все файлы, а только информация о них в главном metadata.csv файле)
- Датасеты проходят восстановление пунктуации в местах, где на аудио присутствуют паузы с помощью Montreal Forced Aligner. Данный инструмент по заданному тексту и аудиофайлу получает информацию о промежутках времени, когда произносятся соответствующие слова.
- Датасеты вручную валидируются на отсутствие явных ошибок.
- Датасеты объединяются в один большой датасет с общим файлом metadata.csv и уникальными на уровне всех данных ID спикеров.  

### 3.6. Перечень возникавших проблем в данных

#### Проблемы, при которых данные удаляются из датасетов
- Несоответствие текста в аудио и в тексте. 
- Очень шумные записи. 
- Неразборчивые произношения слов.  

#### Проблемы, которые должны обрабатываться пайплайном
- Отличная от 44.1kHZ частота дескретизации данных.
- Разное количество каналов записи. Приводится к 1-каналу (mono)
- Разная кодировка аудио. Приводится к 16-bit Signed Integer PCM.
- Слишком длинные аудио. Делятся на 20 секундные куски.
- Слишком мало записей конкретного голоса. Не обрабатывается.
- Слишком много записей конкретного голоса. Обрезается до общей длительности в 1 час. 